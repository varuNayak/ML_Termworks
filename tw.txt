Termwork 1

def aStarAlgo(start_node, stop_node):
    open_set = set(start_node)
    closed_set = set()
    g = {}               #store distance from starting node
    parents = {}         # parents contains an adjacency map of all nodes
    #distance of starting node from itself is zero
    g[start_node] = 0
    #start_node is root node i.e it has no parent nodes
    #so start_node is set to its own parent node
    parents[start_node] = start_node
    while len(open_set) > 0:
        n = None
        #node with lowest f() is found
        for v in open_set:
            if n == None or g[v] + heuristic(v) < g[n] + heuristic(n):
                n = v
        if n == stop_node or Graph_nodes[n] == None:
            pass
        else:
            for (m, weight) in get_neighbors(n):
                #nodes 'm' not in first and last set are added to first
                #n is set its parent
                if m not in open_set and m not in closed_set:
                    open_set.add(m)
                    parents[m] = n
                    g[m] = g[n] + weight
                #for each node m,compare its distance from start i.e g(m) to the
                #from start through n node
                else:
                    if g[m] > g[n] + weight:
                        #update g(m)
                        g[m] = g[n] + weight
                        #change parent of m to n
                        parents[m] = n
                        #if m in closed set,remove and add to open
                        if m in closed_set:
                            closed_set.remove(m)
                            open_set.add(m)
        if n == None:
            print('Path does not exist!')
            return None
        
        # if the current node is the stop_node
        # then we begin reconstructin the path from it to the start_node
        if n == stop_node:
            path = []
            while parents[n] != n:
                path.append(n)
                n = parents[n]
            path.append(start_node)
            path.reverse()
            print('Path found: {}'.format(path))
            return path
        # remove n from the open_list, and add it to closed_list
        # because all of his neighbors were inspected
        open_set.remove(n)
        closed_set.add(n)
    print('Path does not exist!')
    return None

#define fuction to return neighbor and its distance
#from the passed node
def get_neighbors(v):
    if v in Graph_nodes:
        return Graph_nodes[v]
    else:
        return None

def heuristic(n):
    H_dist = {
        'A': 11,
        'B': 6,
        'C': 99,
        'D': 1,
        'E': 7,
        'G': 0,
    }
    return H_dist[n]

#Describe your graph here
Graph_nodes = {
    'A': [('B', 2), ('E', 3)],
    'B': [('A', 2), ('C', 1), ('G', 9)],
    'C': [('B', 1)],
    'D': [('E', 6), ('G', 1)],
    'E': [('A', 3), ('D', 6)],
    'G': [('B', 9), ('D', 1)]
}



aStarAlgo('A', 'G')

--------------------------------------------------------------------------

Termwork 2

graph = {
  '5' : ['3','7'],
  '3' : ['2', '4'],
  '7' : ['8'],
  '2' : [],
  '4' : ['8'],
  '8' : []
}

visited = [] # List for visited nodes.
queue = []     #Initialize a queue
visit = set() # Set to keep track of visited nodes.


def bfs(visited, graph, node): #function for BFS
  visited.append(node)
  queue.append(node)

  while queue:          # Creating loop to visit each node
    m = queue.pop(0) 
    print (m, end = " ") 

    for neighbour in graph[m]:
      if neighbour not in visited:
        visited.append(neighbour)
        queue.append(neighbour)
        
def dfs(visit, graph, node):
    if node not in visit:
        print (node, end = " ")
        visit.add(node)
        for neighbour in graph[node]:
            dfs(visit, graph, neighbour)

# Driver Code
print("Following is the Breadth-First Search")
bfs(visited, graph, '5')
print()
print("Following is the Depth-First Search")
dfs(visit, graph, '5')



---------------------------------------------------------------------------

Termwork 3

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

spam_df=pd.read_csv('emails.csv')
spam_df.head(10)
spam_df.tail(10)

ham=spam_df[spam_df['spam']==0]
spam=spam_df[spam_df['spam']==1]

print('Spam Percentage =',(len(spam)/len(spam_df))*100,'%')
print('Ham Percentage =',(len(ham)/len(spam_df))*100,'%')

from sklearn.feature_extraction.text import CountVectorizer
vectorizer=CountVectorizer()
spamham_countVectorizer=vectorizer.fit_transform(spam_df['text'])

spamham_countVectorizer.shape

label=spam_df['spam']
X=spamham_countVectorizer
y=label

X.shape
y.shape

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)

from sklearn.naive_bayes import MultinomialNB
NB_classifier=MultinomialNB()
NB_classifier.fit(X_train,y_train)

from sklearn.metrics import classification_report,confusion_matrix
y_predict_train=NB_classifier.predict(X_train)
y_predict_train

cm=confusion_matrix(y_train,y_predict_train)
sns.heatmap(cm,annot=True)

y_predict_test=NB_classifier.predict(X_test)
y_predict_test

cm=confusion_matrix(y_test,y_predict_test)
sns.heatmap(cm,annot=True)

print(classification_report(y_test,y_predict_test))

-------------------------------------------------------------------------------

Termwork 4


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import matplotlib.lines as mlines
import math
from sklearn import metrics

from sklearn.model_selection import train_test_split

data = pd.read_csv('Salary_Data.csv')

data['IsHighSalary'] = data['Salary'] >= 100000
data['IsHighSalary'] = pd.get_dummies(data['IsHighSalary'], drop_first=True)
data.tail(10)

X = data['YearsExperience']
X.head()

y = data['IsHighSalary']
y.head()

plt.figure(figsize=(12,6))
sns.pairplot(data,x_vars=['YearsExperience'],y_vars=['IsHighSalary'],height=7,kind='scatter')
plt.xlabel('Years')
plt.ylabel('IsHighSalary')
plt.title('Given years of experience, is the salary >= 100K')
plt.show()

X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=10)

X_train = X_train[:,np.newaxis]
X_test = X_test[:,np.newaxis]

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train,y_train)

y_pred = lr.predict(X_test)

cm = metrics.confusion_matrix(y_test, y_pred)
plt.figure(figsize=(9,9))
sns.heatmap(cm, annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
plt.title('Sklearn model confusion matrix', size = 15);

def model(x):
    exponent = lr.intercept_ + lr.coef_[0] * x
    return 1/(1+math.exp(-exponent))

x = np.arange(1, 11., 0.2)
sig = [model(i) for i in x]


plt.figure(figsize=(12,6))
plt.xlabel('Years')
plt.ylabel('Salary')
plt.title('Salary Prediction')
plt.scatter(data['YearsExperience'], data['IsHighSalary'], c='black')
plt.plot(x,sig)
plt.show()

from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)
accuracy_score(y_test,y_pred)*100

--------------------------------------------------------------------------------

Termwork 5


from sklearn import svm, datasets
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score 
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()

X = iris.data[:, :2] # we only take the first two features
y = iris.target
x_train, x_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)

clf = svm.SVC(kernel='linear', C=1).fit(x_train, y_train)

classifier_predictions = clf.predict(x_test)
print(accuracy_score(y_test, classifier_predictions)*100)

h = 0.02
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
xx.shape


Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)

plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())
plt.title("Linear")
plt.show()

--------------------------------------------------------------------------------

Termwork  6


import numpy as np 
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split  
from sklearn import metrics

names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']

# Read dataset to pandas dataframe
dataset = pd.read_csv("8-dataset.csv", names=names)
X = dataset.iloc[:, :-1]  
y = dataset.iloc[:, -1]
print(X.head())
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.10) 

classifier = KNeighborsClassifier(n_neighbors=5).fit(Xtrain, ytrain) 

ypred = classifier.predict(Xtest)

i = 0
print ("\n-------------------------------------------------------------------------")
print ('%-25s %-25s %-25s' % ('Original Label', 'Predicted Label', 'Correct/Wrong'))
print ("-------------------------------------------------------------------------")
for label in ytest:
    print ('%-25s %-25s' % (label, ypred[i]), end="")
    if (label == ypred[i]):
        print (' %-25s' % ('Correct'))
    else:
        print (' %-25s' % ('Wrong'))
    i = i + 1
print ("-------------------------------------------------------------------------")
print("\nConfusion Matrix:\n", metrics.confusion_matrix(ytest, ypred))  
print ("-------------------------------------------------------------------------")
accuracy = metrics.accuracy_score(ytest,ypred)*100
print('Accuracy of the classifer is %0.2f' % accuracy)
print ("-------------------------------------------------------------------------")

